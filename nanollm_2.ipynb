{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ab8c6a-9272-4459-bf50-f02a55e4c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd727534-f281-4852-b648-9e2342e26e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many number of independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 256 # number of dimensions for embeddings\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "# ----------------------------------------------------------------\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01dacf3-ebb1-4d60-85bf-d3a6b2e97690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f07a6de8930>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9ccc5b-d3c9-449f-90f7-5991ee835faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-10 12:30:09--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.3’\n",
      "\n",
      "100%[======================================>] 1,115,394   --.-K/s   in 0.005s  \n",
      "\n",
      "2024-01-10 12:30:09 (217 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc562572-c769-4822-975d-fe4ff3c1e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be28ca4-eee9-489d-919f-9141b9aba4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All the unique characters that occur in the input\n",
    "# chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)\n",
    "# # create a mapping from characters to integers\n",
    "# stoi = {ch:i for i, ch in enumerate(chars)}    \n",
    "# itos = {i:ch for i, ch in enumerate(chars)}\n",
    "# encode = lambda s: [stoi[c] for c in s]\n",
    "# decode = lambda l: ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d117e65c-e475-44d4-b969-c932c4725a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "text_words = re.split(r\"(\\s|\\n)\", text)\n",
    "\n",
    "words = sorted(list(set(text_words)))\n",
    "vocab_size = len(words)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(words)}    \n",
    "itos = {i:ch for i, ch in enumerate(words)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da0a91ef-3521-4183-8fb3-f2853f79de39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sentencepiece as spm\n",
    "\n",
    "# # train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# # `m.vocab` is just a reference. not used in the segmentation.\n",
    "# spm.SentencePieceTrainer.train('--input=input.txt --model_prefix=m --vocab_size=10000')\n",
    "\n",
    "# # makes segmenter instance and loads the model file (m.model)\n",
    "# sp = spm.SentencePieceProcessor()\n",
    "# sp.load('m.model')\n",
    "\n",
    "# text_words = sp.encode_as_ids(text)\n",
    "\n",
    "# # print(sp.decode_ids([250, 28, 15, 330, 180]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12be0376-779a-4a2b-9346-1fcfa857f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test splits\n",
    "data = torch.tensor(encode(text_words), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # Forst 90% is train and the rest is eval\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# # train and test splits\n",
    "# data = torch.tensor(text_words, dtype=torch.long)\n",
    "# n = int(0.9*len(data)) # Forst 90% is train and the rest is eval\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8fe4e79-b00e-49f9-8489-629e6f7fb9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ad19ce-b12c-4f55-a2c1-011f99d1f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5b86feb-781b-4f79-8ec9-0a2118fbb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single attention head model\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "        \n",
    "    \n",
    "# multiple single heads running in parallel\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple feed forward layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation.\"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get logits and predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75aafc1e-b33d-4b84-8ad5-50dc90394708",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "#create pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8c704ec-7c05-4d4e-a5ac-3781aca32878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.6862, val loss 10.6812\n",
      "step 500: train loss 3.7337, val loss 4.0633\n",
      "step 1000: train loss 3.6551, val loss 4.0685\n",
      "step 1500: train loss 3.5423, val loss 4.0351\n",
      "step 2000: train loss 3.3951, val loss 4.0468\n",
      "step 2500: train loss 3.2038, val loss 4.1116\n",
      "step 3000: train loss 3.0019, val loss 4.1833\n",
      "step 3500: train loss 2.7860, val loss 4.3171\n",
      "step 4000: train loss 2.5715, val loss 4.4908\n",
      "step 4500: train loss 2.3866, val loss 4.6751\n",
      "step 5000: train loss 2.2356, val loss 4.8261\n",
      "step 5500: train loss 2.1073, val loss 4.9531\n",
      "step 6000: train loss 1.9905, val loss 5.0809\n",
      "step 6500: train loss 1.8826, val loss 5.2147\n",
      "step 7000: train loss 1.7855, val loss 5.3216\n",
      "step 7500: train loss 1.6906, val loss 5.4403\n",
      "step 8000: train loss 1.5978, val loss 5.5374\n",
      "step 8500: train loss 1.5108, val loss 5.6361\n",
      "step 9000: train loss 1.4259, val loss 5.7402\n",
      "step 9500: train loss 1.3418, val loss 5.8808\n",
      "CPU times: user 30min 20s, sys: 20min 36s, total: 50min 56s\n",
      "Wall time: 50min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every one in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be4c9b6-5867-40f7-8356-3d8a812b0077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LADY ANNE:\n",
      "What do you not fear?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "I warrant thee fear,--\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "Third Servant:\n",
      "Why, Rivers I spoke dead, I so. but well,\n",
      "\n",
      "DUCHESS Citizen:\n",
      "At this night, of all that prayers is words\n",
      "God's name, hope must have more love.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "I'll be it well: I loved you tell thee,\n",
      "We'll see you all that you have made a husband.\n",
      "Ere you were you knew no such true:\n",
      "Nor would do:\n",
      "\n",
      "Messenger:\n",
      "Your dagger, is enough, you were so brief with being of love.\n",
      "\n",
      "HASTINGS:\n",
      "I mean, your honour's beseech not you:\n",
      "Look grant your times is at the flint, deliver\n",
      "Your brother and very well in this place with either\n",
      "therefore not hold their harms and your hearts\n",
      "For kind, unto him ten thousand and wailing my kin,\n",
      "Who yet plainly me before my mother, I home,\n",
      "Whose being proof to instruct any jot that\n",
      "Desire even to this desperate good as far is\n",
      "forty full of prince: to or that concerns out of their\n",
      "joy waded forth in mine eyes,\n",
      "Of no place are twelve at on them nothing\n",
      "With out superfluous their noble suit\n",
      "Before one against their royal hours\n",
      "They have not such one opposite. You have granted\n",
      "When, their children, and their high lips.\n",
      "And not trouble this young to their bad lives\n",
      "think on their own own gain society,\n",
      "Did I have bought judgment and most them,\n",
      "The last of a\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72055ec8-bc5c-4abd-a212-306d21ab5f24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19548233\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for param in m.parameters():\n",
    "    num_p = param.size().numel()\n",
    "    total_parameters += num_p\n",
    "print(total_parameters)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1e082c2-e7c2-45d0-819b-4db145ba9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate from the model\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# generated_ids = m.generate(context, max_new_tokens=500)[0].tolist()\n",
    "# print(sp.decode_ids(generated_ids))\n",
    "# # print(decode(generated_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928445d-8cb4-4706-80b8-c81d8623a31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636afab-5ea6-4b55-ac41-17a08c7e6f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
